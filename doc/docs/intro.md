---
title: EagleRank Overview
sidebar_position: 1
---

# EagleRank Development Documentation

## 1. System Overview

EagleRank is a production-grade, low-latency feed ranking system designed for a multi-tenant environment. It ingests real-time user activity streams and produces personalized content feeds for users across different applications (tenants). The architecture follows an event-driven microservices design ([kai-waehner.de](https://www.kai-waehner.de/blog/)), [careersatdoordash.com](https://careersatdoordash.com/engineering/), leveraging Apache Kafka for high-throughput event ingestion and Apache Flink for real-time stream processing. All feed generation and ranking logic is performed on demand (a fan-out-on-read model), which simplifies operations and allows dynamic relevance scoring per request ([linkedin.com](https://engineering.linkedin.com/blog/2016/03/lessons-learned-building-linkedin-feed)).

Below is a high-level breakdown of EagleRank's architecture and data flow:

- **Event Ingestion (Kafka):** All user actions (such as posts, likes, follows) and content updates are published to Kafka topics. Kafka serves as the durable, scalable log of events, capable of unifying heterogeneous data sources with high throughput ([careersatdoordash.com](https://careersatdoordash.com/engineering/)). Each tenant's events are tagged with a tenant ID for isolation.
- **Stream Processing (Flink):** Apache Flink consumes the Kafka streams to build and update real-time state:
  - Maintains the social graph (follow relationships) by processing follow/unfollow events.
  - Aggregates engagement signals and content metadata into a feature store.
  - Flink's stateful processing enables on-the-fly transformations and windowed computations with strong consistency guarantees ([kai-waehner.de](https://www.kai-waehner.de/blog/)). For example, Flink can compute rolling engagement metrics (likes, clicks) per content and user in real-time.
- **Processed results** (e.g. updated feature values or graph updates) are written to fast data stores (like Redis and a graph database) for use by the online services.
- **Candidate Generation Service:** A microservice responsible for retrieving a set of candidate content items for a user's feed request. It queries the social graph to get the user's follow list and fetches recent content from those followed entities (and potentially additional sources like trending content or ads). EagleRank primarily uses a fan-out-on-read approach: when a user requests their feed, the service pulls content from the relevant sources on the fly instead of pre-computing feeds for every user. (This strategy was chosen over fan-out-on-write to ease multi-tenant operations and allow on-demand recomputation; LinkedIn similarly found fan-out-on-read easier to operate and iterate on ranking algorithms ([linkedin.com](https://engineering.linkedin.com/blog/2016/03/lessons-learned-building-linkedin-feed)).) The Candidate service ensures only content relevant to the specific tenant and user is retrieved.
- **Ranking Service:** This service scores and ranks the candidate items for the user. It loads the latest user features, item features, and context features from the feature store, then applies a machine learning model to compute a relevance score for each candidate. The highest scoring items are returned as the personalized feed. The Ranking service is optimized for low latency inference – performing feature lookups and model predictions typically within a few milliseconds per item. By handling scoring at request time, EagleRank can easily A/B test and update ranking models or algorithms without reprocessing entire feeds (a key benefit of the on-demand approach ([linkedin.com](https://engineering.linkedin.com/blog/2016/03/lessons-learned-building-linkedin-feed))).
- **Feature Store:** EagleRank uses a feature store to serve precomputed features for users and content items with minimal latency. Features such as user preferences, content popularity, and social signals are continuously updated via Flink and batch jobs. For online serving, these are stored in a fast key-value store (Redis) indexed by tenant and entity ID, which ensures millisecond-level read times ([craft.faire.com](https://craft.faire.com/real-time-ranking-at-faire-part-2-the-feature-store-3f1013d3fe5d)). Both offline batch-computed features and real-time computed features are utilized, combined in a unified store for the ranker ([craft.faire.com](https://craft.faire.com/real-time-ranking-at-faire-part-2-the-feature-store-3f1013d3fe5d)). For example, a user's long-term engagement score (from batch processing) and their real-time session activity (from streams) are both available to the model at serve time.
- **API Gateway (gRPC):** A gRPC API front-end (which can also expose REST endpoints via transcoding) serves as the single entry point for clients. External applications (mobile/web clients for each tenant) call the gateway's Feed API to request personalized feeds. The gateway then fans out requests to the Candidate Generation and Ranking services (or orchestrates them in sequence) over internal gRPC. By using gRPC, we get efficient binary serialization and streamlined service-to-service communication. The gateway also handles authentication, rate limiting, and tenant routing (ensuring a request is mapped to the correct tenant context).
- **Web UI:** A simple web application is provided for internal users (engineers, tenant developers) to visualize and debug the feed results. This UI interacts with EagleRank through the same API Gateway (using gRPC-Web in the browser). It allows testing different user IDs, viewing ranked content, and exploring feature values in real time for debugging and demo purposes.

## Data Flow Summary

When a piece of content is created or a user action happens, an event is sent to Kafka. Flink processing jobs consume these events to update the social graph and features. When a user (of a certain tenant app) opens their feed (via the client or the Web UI), the request goes to the API Gateway, which authenticates the call and identifies the tenant. The Candidate service retrieves recent content for that user (e.g. posts from followed users, or relevant recommendations). The Ranking service then fetches the necessary features and runs the ML model to score the items. The scored list is sorted and returned as the feed response to the client. This whole request/response cycle is designed to be low-latency (on the order of tens of milliseconds), leveraging in-memory caches and efficient algorithms to meet a target of ~10ms p99 service latency. The system scales horizontally at each layer – Kafka partitions for ingestion, Flink operators for processing, and multiple instances of the candidate and ranking microservices – to handle high throughput and large numbers of tenants and users.

(Architecture Diagram:) In lieu of an image, the layout can be described as follows: Kafka clusters sit at the ingestion layer, feeding into a cluster of Flink stream processors that handle real-time updates (with connections to the Feature Store and Social Graph storage for state). On the serving side, client requests hit the gRPC API Gateway, which then communicates with the Candidate Generation service (which queries the Social Graph DB and content repositories) and the Ranking service (which queries the Feature Store and loaded ML model). The Ranking service may call an internal Model Serving component or library (for ONNX runtime) to score content. All services emit metrics to Prometheus and logs/traces to the observability stack. This microservice ecosystem is deployed on Kubernetes (EKS on AWS), with supporting infrastructure like AWS MSK (Managed Kafka), Redis (ElastiCache) for the feature store, and S3 for any offline data storage or model files.

## Quickstart
- **[Setup & Deployment](setup.md)**: How to install and run EagleRank locally or in the cloud.
- **[API Reference](api.md)**: How to integrate with EagleRank's APIs.
- **[Feature Store](feature-store.md)**: Managing and serving features for ranking models.
- **[Ranking Models](ranking-models.md)**: Training, deploying, and serving ML models.

---

For a detailed implementation guide and technology choices, see the [Architecture](architecture.md) section.
